---
title: "Inference"
author: "Andri, Eden, Veronica"
date: "11/19/2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE}
library(dplyr)
source('dataProcessing.R')
library(estimatr)
library(randomForest)
dlexp = loadData()
```


```{r}
#FRT
meanDiff <- function(z, y){
  ZY = data.frame(z)
  ZY$y = y
  treat = filter(ZY, z == 1)
  ctrl = filter(ZY, z == 0)
  
  return (mean(treat$y) - mean(ctrl$y))
}

# Fisher randomization test
frtSim <- function(z, y, M, testStat){
  tSims = vector(, M)
  for (i in 1:M){
    set.seed(i)
    zSim = sample(z)
    tSims[i] = testStat(zSim, y)
  }

  return (tSims)
}

pVal <- function(tObserved, tzSims, M){
  return (length(which(tzSims >= tObserved))/M)
}
```

```{r}
Z = dlexp$Z
Y = dlexp$Y # response, -2:2, could be 1-5, doesnt matter

tObs = meanDiff(Z, Y)

M = 10000
tSims = frtSim(Z, Y, M, meanDiff)
p = pVal(tObs, tSims, M)

print(tObs)
print(p)
```



```{r}
# Neyman

s_2_z <- function(data, z){
  sub_ = filter(data, Z == z)
  n_z = nrow(sub_)

  return (c(var(sub_$Y), n_z))
}

# conservative variance estimate
vHat = function(data){
  s_2_1 = s_2_z(data, 1)
  s_2_0 = s_2_z(data, 0)

  v_hat = (s_2_1[1]/s_2_1[2]) + (s_2_0[1]/s_2_0[2])
  return (v_hat)
}

# normal approx 95% confidence interval
ci <- function(that, vhat){
  return (c(that - 1.96*sqrt(vhat), that + 1.96*sqrt(vhat)))
}


v_hat = vHat(dlexp)
print(ci(tObs, v_hat))
```


```{r}
# discard those who did not get party cue
dParty = filter(dlexp, party.cues == 1, !is.na(pid3)) # what to do with NA, discard for now
Z = dParty$video
Y = dParty$response
tObs = meanDiff(Z, Y)

# what is statistically significant
tSims = frtSim(Z, Y, M, meanDiff)
p = pVal(tObs, tSims, M)

print(tObs)
print(p)


v_hat = vHat(dParty)
print(v_hat)
print(ci(tObs, v_hat))
```
```{r}
# run for each party separately

runParty = function(data, pid){
  dat = filter(data, pid3 == pid)
  tau_hat = meanDiff(dat$Z, dat$Y)
  v_hat = vHat(dat)
  ci_ = ci(tau_hat, v_hat)
  print(tau_hat)
  print(ci_)
  return (c(tau_hat, ci_))
}

neymDem = runParty(dParty, 1) # dem
neymInd = runParty(dParty, 2) # ind
neymRep = runParty(dParty, 3) # rep
# No surprises here, same results as paper


# plot confidence intervals
df = rbind(neymDem, neymInd, neymRep)
df <- as.data.frame(df)

new_column_names <- c("tau_hat", "ci_lower", "ci_upper")
colnames(df) <- new_column_names
df$politicalParty = c("Democrat", "Independent", "Republican")


ggplot(df, aes(politicalParty, tau_hat)) + geom_point() +  
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
  xlab("Political Party") +
  coord_flip()
```


```{r}
# WE WILL NOT INCLUDE THIS
# Run post-stratification with political parties as strata
# only including those who have seen the party clue(more relevant) - we could frame our "research question" as: Does knowing the conclusion from a minipublic have an effect on .... if
# party stance on the issue/policy is known to the individual .... No?


# peng-ding chapter 5
# what to do with NA, discard for now

# Stratification maybe not relevant, since covaraite(political party) split is even in treatment and control

tauHatStrat = function(data, strata_col){
  n = nrow(data)
  strata = unique(data[[strata_col]])
  
  tauHat = 0
  for (k in strata){
    datStrat = filter(data, !!sym(strata_col) == k)
    nk = nrow(datStrat)
    tau_strat = meanDiff(datStrat$Z, datStrat$Y)
    tauHat = tauHat + (nk/n*tau_strat)
  }
    
  return (tauHat)
}

vHatStrat = function(data, strata_col){
  n = nrow(data)
  strata = unique(data[[strata_col]])
  
  v_hat = 0
  for (k in strata){
    datStrat = filter(data, !!sym(strata_col) == k)
    nk = nrow(datStrat)
    v_h_k = vHat(datStrat)
    v_hat = v_hat + ((nk/n)^2)*v_h_k
  }
    
  return (v_hat)
}



tau_hat_s = tauHatStrat(dParty, "pid3")
print(tau_hat_s)

v_hat_s = vHatStrat(dParty, "pid3")
print(v_hat_s)

print(ci(tau_hat_s, v_hat_s))
```

```{r}
# this is crap, dont use!
# start with simple lins estimator
# need to check linearity assumptions later to verify validity of linear lins

# run separately on each political party
linsEst = function(pid){
  df = filter(dParty, pid3 == pid)
  Y = df$Y
  Z = df$Z
  X = model.matrix(~ 0 + income + educ4 + age + white, data = df) %>% scale(center = T, scale=F)
  
  model = lm(Y ~ Z + (X)*(1 + Z), data = df)
  pred = predict(model, df)
  res = df$Y - pred

  plot(density(res))
  return (model)
}


linsEst(1)
linsEst(2)
linsEst(3)

# linearity assumpitons do not hold, do random forest instead
```


```{r}
# begin Lin's here

dParty$white = as.numeric(dParty$white)
dParty$Y_factor = as.factor(dParty$Y)
df = filter(dParty, pid3 == 1)

# shift by centering of other data
centerVars = function(dataCenter, dataShift){
  vars = c("income", "educ4", "age", "white") # covariates to shift
  data_ = dataShift
  for (var in vars){
    data_[, var] <- data_[, var] - mean(dataCenter[, var])
  }
  return (data_)
}

```



```{r}
# Generalized Lin's Estimator with random forest model

# Using classification to predict and then interpret as continuous??
trainModel = function(data){
  model = randomForest(Y_factor ~ income + educ4 + age + white, data = data)
  return (model)
}

# predict for x
# training data: data that model was trained on
predict_calibrate_cross = function(model,  x, leftout_data, training_data){
  # shift left-out data based on centering for training data
  x_shifted = centerVars(training_data, x)
  leftout_data_shifted = centerVars(training_data, leftout_data)
    
  n_d = nrow(leftout_data)
  mu_hat = as.numeric(predict(model, x_shifted))
  l_pred = as.numeric(predict(model, leftout_data_shifted))
  
  l_pred_shift = sum((leftout_data$Y) - l_pred)/n_d
  
  mu_out = mu_hat + l_pred_shift
  return (mu_out)
}


# t_data: training data
# l_data: left-out data
tau_pred_cross = function(control_model, treatment_model,
                          control_t_data, treatment_t_data,
                          control_l_data, treatment_l_data){
  
  n_t = nrow(control_t_data) + nrow(treatment_t_data)
  
  tau = (sum(treatment_t_data$Y) + 
           sum(predict_calibrate_cross(treatment_model, control_t_data, treatment_l_data, treatment_t_data)) -
           sum(control_t_data$Y) -
           sum(predict_calibrate_cross(control_model, treatment_t_data, control_l_data, control_t_data)))/n_t
  
  return (tau)
}

t_pred_cross_total = function(control_model_1, control_model_2,
                              treatment_model_1, treatment_model_2,
                              control_data_1, control_data_2,
                              treatment_data_1, treatment_data_2){
  
  n_1 = nrow(control_data_1) + nrow(treatment_data_1)
  n_2 = nrow(control_data_2) + nrow(treatment_data_2)
  n = n_1 + n_2
  
  t_out = (tau_pred_cross(control_model_2, treatment_model_2,
                          control_data_2, treatment_data_2,
                          control_data_1, treatment_data_1)*(n_1/n)) +
    (tau_pred_cross(control_model_1, treatment_model_1,
                    control_data_1, treatment_data_1,
                    control_data_2, treatment_data_2)*(n_2/n))
  
  return (t_out)
}


get_tau_cross = function(data){
  set.seed(0102)
  split_ind = sample(seq_len(nrow(data)), size = floor(0.5*nrow(data)))
  
  d1 = data[split_ind, ]
  d1_centered = centerVars(d1, d1)
  
  d2 = data[-split_ind, ]
  d2_centered = centerVars(d2, d2)
  
  d1_ctrl = filter(d1, Z == 0)
  d1_ctrl_centered = filter(d1_centered, Z == 0)
  d1_treatment = filter(d1, Z == 1)
  d1_treatment_centered = filter(d1_centered, Z == 1)
  
  d2_ctrl = filter(d2, Z == 0)
  d2_ctrl_centered = filter(d2_centered, Z == 0)
  d2_treatment = filter(d2, Z == 1)
  d2_treatment_centered = filter(d2_centered, Z == 1)
  
  ctrl_model_1 = trainModel(d1_ctrl_centered)
  treatment_model_1 = trainModel(d1_treatment_centered)
  
  ctrl_model_2 = trainModel(d2_ctrl_centered)
  treatment_model_2 = trainModel(d2_treatment_centered)
  
  
  t_pred = t_pred_cross_total(ctrl_model_1, ctrl_model_2,
                              treatment_model_1, treatment_model_2,
                              d1_ctrl, d2_ctrl,
                              d1_treatment, d2_treatment)
  
  return (t_pred)
  
}

# note, this is somewhat volatile with regards to seed for data split, should we do mean of x runs? if yes, how is that treated in bootstrap(same?)
tau_hat_all = get_tau_cross(dParty) 
```

```{r}
# bootstrap variance, this takes a long time to run

boostrapTau = function(data, tau_function, M){
  t = vector(, M)
  for (i in 1:M){
    set.seed(i)
    data_sample = sample_n(data, nrow(data), replace = TRUE)
     t[i] = tau_function(data_sample)
  }
  
  return (t)
}

```



```{r}
bootSampleAll = boostrapTau(dParty, get_tau_cross, 1000)
```

```{r}
hist(bootSampleAll)
ci(tau_hat_all, var(bootSampleAll))
```


```{r}
runLins = function(pid){
  data = filter(dParty, pid3 == pid)
  tau_est = get_tau_cross(data)
  bootSample = boostrapTau(data, get_tau_cross, 1000)
  hist(bootSample)
  
  ci_boot = ci(tau_est, var(bootSample))
  
  return (c(tau_est, ci_boot))
  
}

(l1 = runLins(1)) # dem

# error for ind/dem because of missing levels, how should we fix that? just omit missing levels for some subsets? or do random forest regression instead of classification
(l2 = runLins(2)) # ind
(l3 = runLins(3)) # rep
```

